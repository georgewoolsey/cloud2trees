% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils_rf.R
\name{rf_tune_subsample}
\alias{rf_tune_subsample}
\title{implements steps to mitigate very long run-times when tuning random forests models}
\usage{
rf_tune_subsample(
  predictors,
  response,
  threshold = 14444,
  n_subsamples = 4,
  ntree_try = 44,
  step_factor = 1,
  improve = 0.03
)
}
\arguments{
\item{predictors}{data.frame. predictor variable (x) data}

\item{response}{numeric. vector of response variable (y) data. observations should be ordered to match those in \code{predictors}}

\item{threshold}{numeric. the threshold number of observations, if observations exceed this threshold, subsampling is implemented}

\item{n_subsamples}{numeric. number of times to subsample and tune using \code{\link[randomForest:tuneRF]{randomForest::tuneRF()}}. The most common optimal \code{mtry} is returned from these subsample iterations.}

\item{ntree_try}{numeric. see \code{\link[randomForest:tuneRF]{randomForest::tuneRF()}}}

\item{step_factor}{numeric. see \code{\link[randomForest:tuneRF]{randomForest::tuneRF()}}}

\item{improve}{numeric. see \code{\link[randomForest:tuneRF]{randomForest::tuneRF()}}}
}
\value{
A numeric value to use in the \code{mtry} parameter of \code{\link[randomForest:randomForest]{randomForest::randomForest()}}
}
\description{
\code{rf_tune_subsample()} implements steps to mitigate very long run-times when tuning random forests models.
\code{\link[randomForest:tuneRF]{randomForest::tuneRF()}} enables model tuning by searching for the optimal \code{mtry} parameter (the number of variables randomly sampled as candidates at each split) using a cross-validation approach.
However, computational cost increases significantly with the number of observations as \code{\link[randomForest:tuneRF]{randomForest::tuneRF()}} performs cross-validation internally for each \code{mtry} value it tries.
With 100,000+ observations, each of these cross-validation runs involves building and evaluating many random forest trees, making the process very time-consuming.

The computational cost of random forests is driven by the repeated tree building process,
which involves recursive partitioning, bootstrapping, and feature subset selection.
These operations, when performed on massive datasets, result in a significant computational burden.

\code{rf_tune_subsample()} remedies these issues via:
\itemize{
\item Reducing the \code{ntreeTry} parameter to a smaller value. Tuning will be less precise, but it will finish in a reasonable time. The \code{ntree} parameter can then be increased for the final model.
\item Subsampling. Uses a smaller, representative subsample of the data (e.g., 10-20\% of your data) to find a good \code{mtry} value on the subsample.
}
}
\keyword{internal}
